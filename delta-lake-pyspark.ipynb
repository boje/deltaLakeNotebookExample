{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using Delta Lake 0.4 without Databricks\n",
    "\n",
    "[Delta Lake](https://delta.io/) is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads. But it is **way more** then that!\n",
    "\n",
    "This is a rewritten notebook example from this [blog post](https://databricks.com/blog/2019/10/03/simple-reliable-upserts-and-deletes-on-delta-lake-tables-using-python-apis.html) by Databricks. The intension is to show why Delta Lake is a big deal and how to run Delta Lake without a Databricks services.\n",
    "\n",
    "Delta Lake examples in this notebook:\n",
    "* Convert data to as Delta Lake format\n",
    "* Create Delta Lake table\n",
    "* Spark SQL capabilities\n",
    "* Delete data\n",
    "* Update data\n",
    "* View audit history of table\n",
    "* Merge (union) of two tables which remove duplicates, updates rows and add a new row\n",
    "\n",
    "For testing this docker can be used: ```docker run -it --rm -p 8888:8888 -p 4040:4040 jupyter/pyspark-notebook```\n",
    "\n",
    "### Author\n",
    "Anders Boje Larsen - [alarsen@deloitte.dk](alarsen@deloitte.dk) - [LinkedIn](https://www.linkedin.com/in/andersboje/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Preparation\n",
    "Configure locations for the source file and where the Delta Lake Table will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripdelaysFilePath = \"departuredelays.csv\" \n",
    "pathToEventsTable = \"departureDelays.delta\"\n",
    "flightdata = \"https://raw.githubusercontent.com/drabastomek/learningPySpark/master/Chapter03/flight-data/departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Requirement already up-to-date: pyspark in /Users/alarsen/miniconda3/envs/py36/lib/python3.6/site-packages (2.4.4)\nRequirement already satisfied, skipping upgrade: py4j==0.10.7 in /Users/alarsen/miniconda3/envs/py36/lib/python3.6/site-packages (from pyspark) (0.10.7)\n"
    }
   ],
   "source": [
    "!pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--2019-10-24 20:56:58--  https://raw.githubusercontent.com/drabastomek/learningPySpark/master/Chapter03/flight-data/departuredelays.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)...199.232.40.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.40.133|:443... connected.\nHTTP request sent, awaiting response...200 OK\nLength: 33396236 (32M) [text/plain]\nSaving to: ‘departuredelays.csv.2’\n\ndeparturedelays.csv 100%[===================>]  31.85M  9.79MB/s    in 3.3s    \n\n2019-10-24 20:57:02 (9.79 MB/s) - ‘departuredelays.csv.2’ saved [33396236/33396236]\n\n"
    }
   ],
   "source": [
    "#Download data flight dataset\n",
    "!rm -fr departureDelays.delta\n",
    "!wget https://raw.githubusercontent.com/drabastomek/learningPySpark/master/Chapter03/flight-data/departuredelays.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[('spark.submit.pyFiles', '/Users/alarsen/.ivy2/jars/io.delta_delta-core_2.11-0.4.0.jar,/Users/alarsen/.ivy2/jars/org.antlr_antlr4-4.7.jar,/Users/alarsen/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,/Users/alarsen/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,/Users/alarsen/.ivy2/jars/org.antlr_ST4-4.0.8.jar,/Users/alarsen/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,/Users/alarsen/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,/Users/alarsen/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'), ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'), ('spark.repl.local.jars', 'file:///Users/alarsen/.ivy2/jars/io.delta_delta-core_2.11-0.4.0.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///Users/alarsen/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///Users/alarsen/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///Users/alarsen/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'), ('spark.databricks.delta.retentionDurationCheck.enabled', 'false'), ('spark.master', 'local[*]'), ('spark.submit.deployMode', 'client'), ('spark.files', 'file:///Users/alarsen/.ivy2/jars/io.delta_delta-core_2.11-0.4.0.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///Users/alarsen/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///Users/alarsen/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///Users/alarsen/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'pyspark-shell'), ('spark.jars', 'file:///Users/alarsen/.ivy2/jars/io.delta_delta-core_2.11-0.4.0.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///Users/alarsen/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///Users/alarsen/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///Users/alarsen/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///Users/alarsen/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar')]\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.11:0.4.0 pyspark-shell'\n",
    "\n",
    "sc_conf = SparkConf()\n",
    "sc_conf.set('spark.databricks.delta.retentionDurationCheck.enabled', 'false')\n",
    "sc_conf.set('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "print(sc_conf.getAll())\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(conf=sc_conf)\n",
    "except:\n",
    "    sc = SparkContext(conf=sc_conf)\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create `departureDelays` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "departureDelays = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(tripdelaysFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Save table as Delta Lake (update `pathToEventsTable` to match the following location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "departureDelays.write.format(\"delta\").mode(\"overwrite\").save(pathToEventsTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Load Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_delta = spark.read.format(\"delta\").load(pathToEventsTable)\n",
    "delays_delta.createOrReplaceTempView(\"delays_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get count of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count(1)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1698</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   count(1)\n0      1698"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from delays_delta where origin = 'SEA' and destination = 'SFO'\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Review File System**: Note there are four files initially created as part of the table creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\u001b[1m\u001b[36m_delta_log\u001b[m\u001b[m/\npart-00000-a7d6a844-14a1-46dc-b4cd-b77a0ad81e86-c000.snappy.parquet\npart-00001-8074dff5-6d8c-401a-ae68-e96e7421fa9d-c000.snappy.parquet\npart-00002-b197711b-ac23-4545-9a21-b23f75f2d967-c000.snappy.parquet\npart-00003-d695c1b4-996a-4ccd-ab04-98b2379f8dbb-c000.snappy.parquet\npart-00004-be481f0f-5371-4d12-9610-726e288ffc3d-c000.snappy.parquet\npart-00005-c51aa017-7e57-453c-a152-e1b1402c71e2-c000.snappy.parquet\npart-00006-b213286a-d1dd-4305-b91b-aaa3798cde1d-c000.snappy.parquet\npart-00007-f04adaab-04cb-4a8a-a6ea-9022a31bfe43-c000.snappy.parquet\n"
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deletes\n",
    "With Delta Lake, you can delete data with the Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "deltaTable = DeltaTable.forPath(spark, pathToEventsTable)\n",
    "deltaTable.delete(\"delay < 0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count(1)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>837</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   count(1)\n0       837"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Row Count\n",
    "spark.sql(\"select count(1) from delays_delta where origin = 'SEA' and destination = 'SFO'\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Review File System**: Note that while we deleted early (and on-time) flights, there are now eight files (instead of the four files initially created as part of the table creation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\u001b[1m\u001b[36m_delta_log\u001b[m\u001b[m/\npart-00000-a7d6a844-14a1-46dc-b4cd-b77a0ad81e86-c000.snappy.parquet\npart-00000-eb445dee-f7d4-45e1-b0fd-773120453d92-c000.snappy.parquet\npart-00001-8074dff5-6d8c-401a-ae68-e96e7421fa9d-c000.snappy.parquet\npart-00001-c77c66d0-a97d-4caf-b136-e79d272505fe-c000.snappy.parquet\npart-00002-b197711b-ac23-4545-9a21-b23f75f2d967-c000.snappy.parquet\npart-00002-e6e7e427-a510-4f8d-9bf1-43bfabbb524f-c000.snappy.parquet\npart-00003-39ce1e03-9535-4243-beff-59b336223c6f-c000.snappy.parquet\npart-00003-d695c1b4-996a-4ccd-ab04-98b2379f8dbb-c000.snappy.parquet\npart-00004-be481f0f-5371-4d12-9610-726e288ffc3d-c000.snappy.parquet\npart-00004-d497d609-0b2a-49b9-8bbc-46bc2e48ad04-c000.snappy.parquet\npart-00005-9490e34c-6886-4f8b-80c3-5ae852738196-c000.snappy.parquet\npart-00005-c51aa017-7e57-453c-a152-e1b1402c71e2-c000.snappy.parquet\npart-00006-0fa92e12-eb12-4498-ba6e-0f26e5286f5e-c000.snappy.parquet\npart-00006-b213286a-d1dd-4305-b91b-aaa3798cde1d-c000.snappy.parquet\npart-00007-e6801601-2659-4577-b51b-c9c962ee10b7-c000.snappy.parquet\npart-00007-f04adaab-04cb-4a8a-a6ea-9022a31bfe43-c000.snappy.parquet\n"
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Updates\n",
    "Update flights originating from Detroit (DTW) to now be from Seattle (SEA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.update(\"origin = 'DTW'\", { \"origin\": \"'SEA'\" } ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count(1)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>986</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   count(1)\n0       986"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from delays_delta where origin = 'SEA' and destination = 'SFO'\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### View History\n",
    "View the table history (note the create table, insert, and update operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>version</th>\n      <th>timestamp</th>\n      <th>userId</th>\n      <th>userName</th>\n      <th>operation</th>\n      <th>operationParameters</th>\n      <th>job</th>\n      <th>notebook</th>\n      <th>clusterId</th>\n      <th>readVersion</th>\n      <th>isolationLevel</th>\n      <th>isBlindAppend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2019-10-24 20:57:14</td>\n      <td>None</td>\n      <td>None</td>\n      <td>UPDATE</td>\n      <td>{'predicate': '(origin#1503 = DTW)'}</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2019-10-24 20:57:09</td>\n      <td>None</td>\n      <td>None</td>\n      <td>DELETE</td>\n      <td>{'predicate': '[\"(`delay` &lt; 0)\"]'}</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2019-10-24 20:57:05</td>\n      <td>None</td>\n      <td>None</td>\n      <td>WRITE</td>\n      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   version           timestamp userId userName operation  \\\n0        2 2019-10-24 20:57:14   None     None    UPDATE   \n1        1 2019-10-24 20:57:09   None     None    DELETE   \n2        0 2019-10-24 20:57:05   None     None     WRITE   \n\n                          operationParameters   job notebook clusterId  \\\n0        {'predicate': '(origin#1503 = DTW)'}  None     None      None   \n1          {'predicate': '[\"(`delay` < 0)\"]'}  None     None      None   \n2  {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n\n   readVersion isolationLevel  isBlindAppend  \n0          1.0           None          False  \n1          0.0           None          False  \n2          NaN           None          False  "
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.history().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate counts for each version of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "SEA -> SFO Counts: Create Table: 1698, Delete: 837, Update: 986\n"
    }
   ],
   "source": [
    "dfv0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"departureDelays.delta\")\n",
    "dfv1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"departureDelays.delta\")\n",
    "dfv2 = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"departureDelays.delta\")\n",
    "\n",
    "cnt0 = dfv0.where(\"origin = 'SEA'\").where(\"destination = 'SFO'\").count()\n",
    "cnt1 = dfv1.where(\"origin = 'SEA'\").where(\"destination = 'SFO'\").count()\n",
    "cnt2 = dfv2.where(\"origin = 'SEA'\").where(\"destination = 'SFO'\").count()\n",
    "\n",
    "print(\"SEA -> SFO Counts: Create Table: %s, Delete: %s, Update: %s\" % (cnt0, cnt1, cnt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Review File System**: Note the number of files based on the preceding operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\u001b[1m\u001b[36m_delta_log\u001b[m\u001b[m/\npart-00000-a7d6a844-14a1-46dc-b4cd-b77a0ad81e86-c000.snappy.parquet\npart-00000-eb445dee-f7d4-45e1-b0fd-773120453d92-c000.snappy.parquet\npart-00000-ec5df012-c7fd-4c34-b3bc-030be8764c2c-c000.snappy.parquet\npart-00001-8074dff5-6d8c-401a-ae68-e96e7421fa9d-c000.snappy.parquet\npart-00001-a513c0f6-419a-4c5e-8c8b-6bbcc485bd62-c000.snappy.parquet\npart-00001-c77c66d0-a97d-4caf-b136-e79d272505fe-c000.snappy.parquet\npart-00002-3f59f723-91d5-4be6-bae4-1401130be4fc-c000.snappy.parquet\npart-00002-b197711b-ac23-4545-9a21-b23f75f2d967-c000.snappy.parquet\npart-00002-e6e7e427-a510-4f8d-9bf1-43bfabbb524f-c000.snappy.parquet\npart-00003-39ce1e03-9535-4243-beff-59b336223c6f-c000.snappy.parquet\npart-00003-d695c1b4-996a-4ccd-ab04-98b2379f8dbb-c000.snappy.parquet\npart-00004-be481f0f-5371-4d12-9610-726e288ffc3d-c000.snappy.parquet\npart-00004-d497d609-0b2a-49b9-8bbc-46bc2e48ad04-c000.snappy.parquet\npart-00005-9490e34c-6886-4f8b-80c3-5ae852738196-c000.snappy.parquet\npart-00005-c51aa017-7e57-453c-a152-e1b1402c71e2-c000.snappy.parquet\npart-00006-0fa92e12-eb12-4498-ba6e-0f26e5286f5e-c000.snappy.parquet\npart-00006-b213286a-d1dd-4305-b91b-aaa3798cde1d-c000.snappy.parquet\npart-00007-e6801601-2659-4577-b51b-c9c962ee10b7-c000.snappy.parquet\npart-00007-f04adaab-04cb-4a8a-a6ea-9022a31bfe43-c000.snappy.parquet\n"
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vacuum\n",
    "Remove older data (by default 7 days) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.vacuum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\u001b[1m\u001b[36m_delta_log\u001b[m\u001b[m/\npart-00000-eb445dee-f7d4-45e1-b0fd-773120453d92-c000.snappy.parquet\npart-00000-ec5df012-c7fd-4c34-b3bc-030be8764c2c-c000.snappy.parquet\npart-00001-a513c0f6-419a-4c5e-8c8b-6bbcc485bd62-c000.snappy.parquet\npart-00001-c77c66d0-a97d-4caf-b136-e79d272505fe-c000.snappy.parquet\npart-00002-3f59f723-91d5-4be6-bae4-1401130be4fc-c000.snappy.parquet\npart-00003-39ce1e03-9535-4243-beff-59b336223c6f-c000.snappy.parquet\npart-00006-0fa92e12-eb12-4498-ba6e-0f26e5286f5e-c000.snappy.parquet\npart-00007-e6801601-2659-4577-b51b-c9c962ee10b7-c000.snappy.parquet\n"
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "And let's not forget, Delta Lake 0.4.0 also includes `MERGE` in the Python API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge\n",
    "Let's merge another table with the `departureDelays` table with [data deduplication](https://docs.delta.io/0.4.0/delta-update.html#data-deduplication-when-writing-into-delta-tables).  Let's start by viewing data that will be impacted by the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>delay</th>\n      <th>distance</th>\n      <th>origin</th>\n      <th>destination</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1010521</td>\n      <td>0</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1010710</td>\n      <td>31</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1010730</td>\n      <td>5</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1010955</td>\n      <td>104</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      date  delay  distance origin destination\n0  1010521      0       590    SEA         SFO\n1  1010710     31       590    SEA         SFO\n2  1010730      5       590    SEA         SFO\n3  1010955    104       590    SEA         SFO"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from delays_delta where origin = 'SEA' and destination = 'SFO' and date like '1010%' order by date limit 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Next, let's create our `merge_table` which contains three rows:\n",
    "* 1010710: this row is a duplicate\n",
    "* 1010521: this row will be updated with a new delay value\n",
    "* 1010822: this is a new row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>delay</th>\n      <th>distance</th>\n      <th>origin</th>\n      <th>destination</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1010521</td>\n      <td>10</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1010710</td>\n      <td>31</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1010832</td>\n      <td>31</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      date  delay  distance origin destination\n0  1010521     10       590    SEA         SFO\n1  1010710     31       590    SEA         SFO\n2  1010832     31       590    SEA         SFO"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [(1010521, 10, 590, 'SEA', 'SFO'), (1010710, 31, 590, 'SEA', 'SFO'), (1010832, 31, 590, 'SEA', 'SFO')]\n",
    "cols = ['date', 'delay', 'distance', 'origin', 'destination']\n",
    "merge_table = spark.createDataFrame(items, cols)\n",
    "merge_table.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's run our merge statement that will handle the duplicates, updates, and add a new row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.alias(\"flights\") \\\n",
    "    .merge(merge_table.alias(\"updates\"),\"flights.date = updates.date\") \\\n",
    "    .whenMatchedUpdate(set = { \"delay\" : \"updates.delay\" } ) \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>delay</th>\n      <th>distance</th>\n      <th>origin</th>\n      <th>destination</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1010521</td>\n      <td>10</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1010710</td>\n      <td>31</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1010730</td>\n      <td>5</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1010832</td>\n      <td>31</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1010955</td>\n      <td>104</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      date  delay  distance origin destination\n0  1010521     10       590    SEA         SFO\n1  1010710     31       590    SEA         SFO\n2  1010730      5       590    SEA         SFO\n3  1010832     31       590    SEA         SFO\n4  1010955    104       590    SEA         SFO"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from delays_delta where origin = 'SEA' and destination = 'SFO' and date like '1010%' order by date limit 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "As noted in the previous cells, notice the following:\n",
    "* There is only one row for the date `1010710` as `merge` automatically takes care of **data deduplication**\n",
    "* The row for the date `1010521` has the `delay` value **updated** from 0 to 10.\n",
    "* The row for the date `1010832` has been added as this date did not exist, hence it was **inserted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}