{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using Delta Lake 0.4 without Databricks\n",
    "\n",
    "[Delta Lake](https://delta.io/) is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads. But it is **way more** then that!\n",
    "\n",
    "This is a rewritten notebook example from this [blog post](https://databricks.com/blog/2019/10/03/simple-reliable-upserts-and-deletes-on-delta-lake-tables-using-python-apis.html) by Databricks. The intension is to show why Delta Lake is a big deal and how to run Delta Lake without a Databricks services.\n",
    "\n",
    "Delta Lake examples in this notebook:\n",
    "* Convert data to as Delta Lake format\n",
    "* Create Delta Lake table\n",
    "* Spark SQL capabilities\n",
    "* Delete data\n",
    "* Update data\n",
    "* View audit history of table\n",
    "* Merge (union) of two tables which remove duplicates, updates rows and add a new row\n",
    "\n",
    "For testing this docker can be used: ```docker run -it --rm -p 8888:8888 -p 4040:4040 jupyter/pyspark-notebook```\n",
    "\n",
    "### Author\n",
    "Anders Boje Larsen - [alarsen@deloitte.dk](alarsen@deloitte.dk) - [LinkedIn](https://www.linkedin.com/in/andersboje/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Preparation\n",
    "Configure locations for the source file and where the Delta Lake Table will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripdelaysFilePath = \"departuredelays.csv\" \n",
    "pathToEventsTable = \"departureDelays.delta\"\n",
    "flightdata = \"https://raw.githubusercontent.com/drabastomek/learningPySpark/master/Chapter03/flight-data/departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Requirement already up-to-date: pyspark in /Users/alarsen/miniconda3/lib/python3.7/site-packages (2.4.4)\nRequirement already satisfied, skipping upgrade: py4j==0.10.7 in /Users/alarsen/miniconda3/lib/python3.7/site-packages (from pyspark) (0.10.7)\n"
    }
   ],
   "source": [
    "!pip install --upgrade pyspark\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--2019-10-24 21:17:48--  https://raw.githubusercontent.com/drabastomek/learningPySpark/master/Chapter03/flight-data/departuredelays.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)...199.232.40.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.40.133|:443...connected.\nHTTP request sent, awaiting response...200 OK\nLength: 33396236 (32M) [text/plain]\nSaving to: ‘departuredelays.csv.5’\n\ndeparturedelays.csv 100%[===================>]  31.85M  9.27MB/s    in 3.5s    \n\n2019-10-24 21:17:54 (9.10 MB/s) - ‘departuredelays.csv.5’ saved [33396236/33396236]\n\n"
    }
   ],
   "source": [
    "#Download data flight dataset\n",
    "!rm -fr departureDelays.delta\n",
    "!wget https://raw.githubusercontent.com/drabastomek/learningPySpark/master/Chapter03/flight-data/departuredelays.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.11:0.4.0 pyspark-shell'\n",
    "\n",
    "sc_conf = SparkConf()\n",
    "sc_conf.set('spark.databricks.delta.retentionDurationCheck.enabled', 'false')\n",
    "sc_conf.set('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(conf=sc_conf)\n",
    "except:\n",
    "    sc = SparkContext(conf=sc_conf)\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create `departureDelays` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "departureDelays = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(tripdelaysFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Save table as Delta Lake (update `pathToEventsTable` to match the following location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "departureDelays.write.format(\"delta\").mode(\"overwrite\").save(pathToEventsTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Load Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_delta = spark.read.format(\"delta\").load(pathToEventsTable)\n",
    "delays_delta.createOrReplaceTempView(\"delays_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get count of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count(1)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1698</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   count(1)\n0      1698"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from delays_delta where origin = 'SEA' and destination = 'SFO'\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Review File System**: Note there are four files initially created as part of the table creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\u001b[1m\u001b[36m_delta_log\u001b[m\u001b[m/\npart-00000-17468cb5-af06-4072-8a84-8a394ecfd7db-c000.snappy.parquet\npart-00001-de10ca1b-7c5a-4e7b-9f54-b82583083a04-c000.snappy.parquet\npart-00002-8e8b25b0-4a3d-4ed8-aec1-1d64e51dec1e-c000.snappy.parquet\npart-00003-a4556043-2fbf-436a-9f10-92d4f86b666f-c000.snappy.parquet\npart-00004-47f3ed39-4806-400d-8990-db6a3887f171-c000.snappy.parquet\npart-00005-368b37ed-02cf-45f6-9d53-1f2e41041cd5-c000.snappy.parquet\npart-00006-3c521f83-7f0f-41ae-b6c4-b7306813eec3-c000.snappy.parquet\npart-00007-0c346081-4a64-44f8-9368-44d33d126348-c000.snappy.parquet\n"
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deletes\n",
    "With Delta Lake, you can delete data with the Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "deltaTable = DeltaTable.forPath(spark, pathToEventsTable)\n",
    "deltaTable.delete(\"delay < 0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count(1)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>837</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   count(1)\n0       837"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Row Count\n",
    "spark.sql(\"select count(1) from delays_delta where origin = 'SEA' and destination = 'SFO'\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Review File System**: Note that while we deleted early (and on-time) flights, there are now eight files (instead of the four files initially created as part of the table creation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\u001b[1m\u001b[36m_delta_log\u001b[m\u001b[m/\npart-00000-17468cb5-af06-4072-8a84-8a394ecfd7db-c000.snappy.parquet\npart-00000-b0252edb-5ab7-45f4-abe3-f86c61e1bb82-c000.snappy.parquet\npart-00001-828c7e25-402b-4ba8-ba0c-3e96c5499a6c-c000.snappy.parquet\npart-00001-de10ca1b-7c5a-4e7b-9f54-b82583083a04-c000.snappy.parquet\npart-00002-239c1673-bab9-45df-9400-7ccee3f09ae7-c000.snappy.parquet\npart-00002-8e8b25b0-4a3d-4ed8-aec1-1d64e51dec1e-c000.snappy.parquet\npart-00003-a4556043-2fbf-436a-9f10-92d4f86b666f-c000.snappy.parquet\npart-00003-afec5f76-09ad-4439-bf8f-7adcd52bb09c-c000.snappy.parquet\npart-00004-47f3ed39-4806-400d-8990-db6a3887f171-c000.snappy.parquet\npart-00004-d90ca5bb-b673-4cdc-aa1b-e5402fd20b39-c000.snappy.parquet\npart-00005-368b37ed-02cf-45f6-9d53-1f2e41041cd5-c000.snappy.parquet\npart-00005-a9d0a35f-5af8-4365-82a1-a47d42c13909-c000.snappy.parquet\npart-00006-2a90baca-f0b9-42d6-bb9d-3d8e7e939502-c000.snappy.parquet\npart-00006-3c521f83-7f0f-41ae-b6c4-b7306813eec3-c000.snappy.parquet\npart-00007-0c346081-4a64-44f8-9368-44d33d126348-c000.snappy.parquet\npart-00007-fed7d610-246e-4f66-a404-2198e862d3b0-c000.snappy.parquet\n"
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Updates\n",
    "Update flights originating from Detroit (DTW) to now be from Seattle (SEA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.update(\"origin = 'DTW'\", { \"origin\": \"'SEA'\" } ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count(1)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>986</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   count(1)\n0       986"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from delays_delta where origin = 'SEA' and destination = 'SFO'\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### View History\n",
    "View the table history (note the create table, insert, and update operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>version</th>\n      <th>timestamp</th>\n      <th>userId</th>\n      <th>userName</th>\n      <th>operation</th>\n      <th>operationParameters</th>\n      <th>job</th>\n      <th>notebook</th>\n      <th>clusterId</th>\n      <th>readVersion</th>\n      <th>isolationLevel</th>\n      <th>isBlindAppend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2019-10-24 21:18:08</td>\n      <td>None</td>\n      <td>None</td>\n      <td>UPDATE</td>\n      <td>{'predicate': '(origin#1503 = DTW)'}</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2019-10-24 21:18:03</td>\n      <td>None</td>\n      <td>None</td>\n      <td>DELETE</td>\n      <td>{'predicate': '[\"(`delay` &lt; 0)\"]'}</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2019-10-24 21:17:58</td>\n      <td>None</td>\n      <td>None</td>\n      <td>WRITE</td>\n      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   version           timestamp userId userName operation  \\\n0        2 2019-10-24 21:18:08   None     None    UPDATE   \n1        1 2019-10-24 21:18:03   None     None    DELETE   \n2        0 2019-10-24 21:17:58   None     None     WRITE   \n\n                          operationParameters   job notebook clusterId  \\\n0        {'predicate': '(origin#1503 = DTW)'}  None     None      None   \n1          {'predicate': '[\"(`delay` < 0)\"]'}  None     None      None   \n2  {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n\n   readVersion isolationLevel  isBlindAppend  \n0          1.0           None          False  \n1          0.0           None          False  \n2          NaN           None          False  "
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.history().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate counts for each version of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "SEA -> SFO Counts: Create Table: 1698, Delete: 837, Update: 986\n"
    }
   ],
   "source": [
    "dfv0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"departureDelays.delta\")\n",
    "dfv1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"departureDelays.delta\")\n",
    "dfv2 = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"departureDelays.delta\")\n",
    "\n",
    "cnt0 = dfv0.where(\"origin = 'SEA'\").where(\"destination = 'SFO'\").count()\n",
    "cnt1 = dfv1.where(\"origin = 'SEA'\").where(\"destination = 'SFO'\").count()\n",
    "cnt2 = dfv2.where(\"origin = 'SEA'\").where(\"destination = 'SFO'\").count()\n",
    "\n",
    "print(\"SEA -> SFO Counts: Create Table: %s, Delete: %s, Update: %s\" % (cnt0, cnt1, cnt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Review File System**: Note the number of files based on the preceding operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\u001b[1m\u001b[36m_delta_log\u001b[m\u001b[m/\npart-00000-149d02e6-b092-468e-8dc7-e09a0dcb5623-c000.snappy.parquet\npart-00000-17468cb5-af06-4072-8a84-8a394ecfd7db-c000.snappy.parquet\npart-00000-b0252edb-5ab7-45f4-abe3-f86c61e1bb82-c000.snappy.parquet\npart-00001-5c1c47e0-712d-4bbc-bfc4-85b3cc7339df-c000.snappy.parquet\npart-00001-828c7e25-402b-4ba8-ba0c-3e96c5499a6c-c000.snappy.parquet\npart-00001-de10ca1b-7c5a-4e7b-9f54-b82583083a04-c000.snappy.parquet\npart-00002-239c1673-bab9-45df-9400-7ccee3f09ae7-c000.snappy.parquet\npart-00002-8e8b25b0-4a3d-4ed8-aec1-1d64e51dec1e-c000.snappy.parquet\npart-00002-99bc922b-eb2f-42bb-b156-6cecffd12be9-c000.snappy.parquet\npart-00003-a4556043-2fbf-436a-9f10-92d4f86b666f-c000.snappy.parquet\npart-00003-afec5f76-09ad-4439-bf8f-7adcd52bb09c-c000.snappy.parquet\npart-00004-47f3ed39-4806-400d-8990-db6a3887f171-c000.snappy.parquet\npart-00004-d90ca5bb-b673-4cdc-aa1b-e5402fd20b39-c000.snappy.parquet\npart-00005-368b37ed-02cf-45f6-9d53-1f2e41041cd5-c000.snappy.parquet\npart-00005-a9d0a35f-5af8-4365-82a1-a47d42c13909-c000.snappy.parquet\npart-00006-2a90baca-f0b9-42d6-bb9d-3d8e7e939502-c000.snappy.parquet\npart-00006-3c521f83-7f0f-41ae-b6c4-b7306813eec3-c000.snappy.parquet\npart-00007-0c346081-4a64-44f8-9368-44d33d126348-c000.snappy.parquet\npart-00007-fed7d610-246e-4f66-a404-2198e862d3b0-c000.snappy.parquet\n"
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vacuum\n",
    "Remove older data (by default 7 days) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.vacuum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\u001b[1m\u001b[36m_delta_log\u001b[m\u001b[m/\npart-00000-149d02e6-b092-468e-8dc7-e09a0dcb5623-c000.snappy.parquet\npart-00000-b0252edb-5ab7-45f4-abe3-f86c61e1bb82-c000.snappy.parquet\npart-00001-5c1c47e0-712d-4bbc-bfc4-85b3cc7339df-c000.snappy.parquet\npart-00001-828c7e25-402b-4ba8-ba0c-3e96c5499a6c-c000.snappy.parquet\npart-00002-99bc922b-eb2f-42bb-b156-6cecffd12be9-c000.snappy.parquet\npart-00003-afec5f76-09ad-4439-bf8f-7adcd52bb09c-c000.snappy.parquet\npart-00006-2a90baca-f0b9-42d6-bb9d-3d8e7e939502-c000.snappy.parquet\npart-00007-fed7d610-246e-4f66-a404-2198e862d3b0-c000.snappy.parquet\n"
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "And let's not forget, Delta Lake 0.4.0 also includes `MERGE` in the Python API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge\n",
    "Let's merge another table with the `departureDelays` table with [data deduplication](https://docs.delta.io/0.4.0/delta-update.html#data-deduplication-when-writing-into-delta-tables).  Let's start by viewing data that will be impacted by the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>delay</th>\n      <th>distance</th>\n      <th>origin</th>\n      <th>destination</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1010521</td>\n      <td>0</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1010710</td>\n      <td>31</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1010730</td>\n      <td>5</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1010955</td>\n      <td>104</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      date  delay  distance origin destination\n0  1010521      0       590    SEA         SFO\n1  1010710     31       590    SEA         SFO\n2  1010730      5       590    SEA         SFO\n3  1010955    104       590    SEA         SFO"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from delays_delta where origin = 'SEA' and destination = 'SFO' and date like '1010%' order by date limit 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Next, let's create our `merge_table` which contains three rows:\n",
    "* 1010710: this row is a duplicate\n",
    "* 1010521: this row will be updated with a new delay value\n",
    "* 1010822: this is a new row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>delay</th>\n      <th>distance</th>\n      <th>origin</th>\n      <th>destination</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1010521</td>\n      <td>10</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1010710</td>\n      <td>31</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1010832</td>\n      <td>31</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      date  delay  distance origin destination\n0  1010521     10       590    SEA         SFO\n1  1010710     31       590    SEA         SFO\n2  1010832     31       590    SEA         SFO"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [(1010521, 10, 590, 'SEA', 'SFO'), (1010710, 31, 590, 'SEA', 'SFO'), (1010832, 31, 590, 'SEA', 'SFO')]\n",
    "cols = ['date', 'delay', 'distance', 'origin', 'destination']\n",
    "merge_table = spark.createDataFrame(items, cols)\n",
    "merge_table.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's run our merge statement that will handle the duplicates, updates, and add a new row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.alias(\"flights\") \\\n",
    "    .merge(merge_table.alias(\"updates\"),\"flights.date = updates.date\") \\\n",
    "    .whenMatchedUpdate(set = { \"delay\" : \"updates.delay\" } ) \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>delay</th>\n      <th>distance</th>\n      <th>origin</th>\n      <th>destination</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1010521</td>\n      <td>10</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1010710</td>\n      <td>31</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1010730</td>\n      <td>5</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1010832</td>\n      <td>31</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1010955</td>\n      <td>104</td>\n      <td>590</td>\n      <td>SEA</td>\n      <td>SFO</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      date  delay  distance origin destination\n0  1010521     10       590    SEA         SFO\n1  1010710     31       590    SEA         SFO\n2  1010730      5       590    SEA         SFO\n3  1010832     31       590    SEA         SFO\n4  1010955    104       590    SEA         SFO"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from delays_delta where origin = 'SEA' and destination = 'SFO' and date like '1010%' order by date limit 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "As noted in the previous cells, notice the following:\n",
    "* There is only one row for the date `1010710` as `merge` automatically takes care of **data deduplication**\n",
    "* The row for the date `1010521` has the `delay` value **updated** from 0 to 10.\n",
    "* The row for the date `1010832` has been added as this date did not exist, hence it was **inserted**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}